{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required packages and downloading dataset"
      ],
      "metadata": {
        "id": "TLEDxeTfq2KB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6YfsiOyQvNU",
        "outputId": "b6808943-1848-4d46-cbf0-49c0d834c7e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets -q\n",
        "!pip install kaggle -q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle && mv kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "butsyTlPYFZY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download kingburrito666/shakespeare-plays\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tvh86E9RhAs",
        "outputId": "2b918a03-124d-4b63-b47a-626fcbb3a1b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading shakespeare-plays.zip to /content\n",
            "\r  0% 0.00/4.55M [00:00<?, ?B/s]\n",
            "\r100% 4.55M/4.55M [00:00<00:00, 72.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip shakespeare-plays.zip -d shakespeare_dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SzgnV72YYu7",
        "outputId": "0f15df8b-ddb6-418d-ee23-4b22546c8fde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  shakespeare-plays.zip\n",
            "  inflating: shakespeare_dataset/Shakespeare_data.csv  \n",
            "  inflating: shakespeare_dataset/alllines.txt  \n",
            "  inflating: shakespeare_dataset/william-shakespeare-black-silhouette.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3y8ekybL01b",
        "outputId": "6c0a8d78-b1e3-4453-be93-643629354f2a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "b56ui-L7q_aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/shakespeare_dataset/Shakespeare_data.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "HsYUQxUeY0S8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'PlayerLine' column\n",
        "player_lines = df['PlayerLine'].dropna()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_lines, test_lines = train_test_split(player_lines, test_size=0.1, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VESFt_8bY8El"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines_to_save = train_lines[:10000]"
      ],
      "metadata": {
        "id": "Fwk6dRXtbVEp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lines_to_save = test_lines[:3000]"
      ],
      "metadata": {
        "id": "2R792K25bkyW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the train and test sets to separate files\n",
        "train_lines_to_save.to_csv('train_dataset.txt', index=False, header=False)\n",
        "test_lines_to_save.to_csv('test_dataset.txt', index=False, header=False)"
      ],
      "metadata": {
        "id": "mqw9Yt0EbTYp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune GPT-2"
      ],
      "metadata": {
        "id": "6TbiAGsdrO65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "def fine_tune_shakespeare(dataset_path, model_checkpoint='gpt2', epochs=40):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_checkpoint)\n",
        "\n",
        "    dataset = TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=dataset_path,\n",
        "        block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./GPT2_shakespeare\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=50,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = fine_tune_shakespeare('train_dataset.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtZiW_qIcbrg",
        "outputId": "e49cd439-49f6-4bcb-e5b6-d93efc807fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model and tokenizer"
      ],
      "metadata": {
        "id": "PBPHcDqLraDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Save the model and the tokenizer\n",
        "model.save_pretrained(\"./gpt2_shakespeare\")\n",
        "tokenizer.save_pretrained(\"./gpt2_shakespeare\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMB92qpjd-l0",
        "outputId": "fe52c85d-0db1-4bf3-ab64-e2f68923e9a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2_shakespeare/tokenizer_config.json',\n",
              " './gpt2_shakespeare/special_tokens_map.json',\n",
              " './gpt2_shakespeare/vocab.json',\n",
              " './gpt2_shakespeare/merges.txt',\n",
              " './gpt2_shakespeare/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "fXzt0HP-rfmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_shakespeare\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_shakespeare\")\n"
      ],
      "metadata": {
        "id": "7ZHbs-Zhc5bU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lines.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H1H1tTDVrvbQ",
        "outputId": "cea4d06d-2970-4c02-eb1b-8c4e70e538f7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That hath deprived me of your grace and favour,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = test_lines.iloc[0]\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=200, num_return_sequences=1)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcZUiRUr8sZ",
        "outputId": "effc6a62-8576-4cca-bfce-7635020668c5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That hath deprived me of your grace and favour, I will\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your tongue,\"\n",
            "\"And, with a flourish of your tongue,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n",
            "\"And, with a flourish of your hand,\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Approach : Using a character-based RNN"
      ],
      "metadata": {
        "id": "B1x7Y21osYUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "_HjkNNqfsdrT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(\" \".join(train_lines)))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ_CARcftXVh",
        "outputId": "00a19b5b-53d1-4a9e-9107-369c817d0359"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(train_lines)"
      ],
      "metadata": {
        "id": "UIGPohaWtq0b"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training, we need to convert the strings to a numerical representation.\n",
        "\n",
        "The `preprocessing.StringLookup` layer can convert each character into a numeric ID"
      ],
      "metadata": {
        "id": "pKBRbWbmP2Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "T48EIRuDuts7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `preprocessing.StringLookup(..., invert=True)`."
      ],
      "metadata": {
        "id": "EgmLjhiCQCa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "pm0Lod87wVRG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "-dhTtBwYwZFd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdc4oOSTwd7D",
        "outputId": "6523adae-d105-4095-c72e-4adf38990547"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3930395,), dtype=int64, numpy=array([44, 58, 69, ..., 52, 54, 10])>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So we break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this, we first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ],
      "metadata": {
        "id": "sCAtQc78QfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "sEu8ZNv2wiPe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIzLrYbI736t",
        "outputId": "1685f8bc-ca63-46e1-9cff-8a25d84a1a42"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "metadata": {
        "id": "XVIkyY-8wmBT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "4Zf17ovNwsD9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For training we'll need a dataset of `(input, label)` pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ],
      "metadata": {
        "id": "tl1l_d8wc6bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "NXU9NqCnw2Bi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "gcPUxoIew4Y0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWn5g0kAw7GT",
        "outputId": "f1457c41-4986-4f6e-9a49-1cb685c4f1e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"Without the king's will or the state's allowance, And by me, had not our hap been bad. This tribute \"\n",
            "Target: b\"ithout the king's will or the state's allowance, And by me, had not our hap been bad. This tribute f\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ],
      "metadata": {
        "id": "zQnehVHedtBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eWKBN3Hw9dV",
        "outputId": "a77b47a2-9cf8-4ee2-fd92-b3b29b6125a5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Build Model"
      ],
      "metadata": {
        "id": "IaFBwbxmeAum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units`.\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ],
      "metadata": {
        "id": "I8hnk3T6eFmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "RnSPaO1bw_hZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "5D3k46c4xA3y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:"
      ],
      "metadata": {
        "id": "b3yjtchzedIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "gOb3O7FuxCwQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model2(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wkT9FoG8efC",
        "outputId": "85b71705-6b45-4d75-bb35-b8b494f2b487"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 76) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qBAjF9axFoO",
        "outputId": "7fbffe53-dbd2-487e-909c-3b28ccd32e76"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  19456     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  77900     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4035660 (15.39 MB)\n",
            "Trainable params: 4035660 (15.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the model"
      ],
      "metadata": {
        "id": "ZORUzdWie7Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "Attaching an optimizer, and a loss function.\n",
        "\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the `from_logits` flag."
      ],
      "metadata": {
        "id": "Q1AzjkOPe5PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "zcjG4cLQxKrM"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "lXqmOX17xON-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "Bz5DnMNUxQSZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "vwKZ71VLxUK6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model2.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlgTuwisxWWI",
        "outputId": "67ac8f5a-0fae-4be9-9ae5-5e1adf5ff2a9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "608/608 [==============================] - 40s 54ms/step - loss: 2.1052\n",
            "Epoch 2/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.5387\n",
            "Epoch 3/20\n",
            "608/608 [==============================] - 36s 57ms/step - loss: 1.4169\n",
            "Epoch 4/20\n",
            "608/608 [==============================] - 35s 54ms/step - loss: 1.3581\n",
            "Epoch 5/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.3177\n",
            "Epoch 6/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.2864\n",
            "Epoch 7/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.2589\n",
            "Epoch 8/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.2335\n",
            "Epoch 9/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.2096\n",
            "Epoch 10/20\n",
            "608/608 [==============================] - 36s 55ms/step - loss: 1.1865\n",
            "Epoch 11/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.1654\n",
            "Epoch 12/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.1453\n",
            "Epoch 13/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.1273\n",
            "Epoch 14/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.1123\n",
            "Epoch 15/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.0990\n",
            "Epoch 16/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.0880\n",
            "Epoch 17/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.0799\n",
            "Epoch 18/20\n",
            "608/608 [==============================] - 35s 55ms/step - loss: 1.0735\n",
            "Epoch 19/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.0683\n",
            "Epoch 20/20\n",
            "608/608 [==============================] - 36s 56ms/step - loss: 1.0653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate text"
      ],
      "metadata": {
        "id": "z8VTIN3yf3If"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as we execute it"
      ],
      "metadata": {
        "id": "DeLite78f7ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following makes a single step prediction:"
      ],
      "metadata": {
        "id": "PW38nV7cgDVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    print(skip_ids)\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "vQgzEaIx_noN"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beautiful_print(text):\n",
        "    text_str = text.numpy()[0].decode(\"utf-8\")\n",
        "    words = text_str.split()\n",
        "\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    word_count = 0\n",
        "\n",
        "    for word in words:\n",
        "        current_line.append(word)\n",
        "        word_count += 1\n",
        "        if word.endswith(('.', '?', '!')) and 20 <= word_count <= 30:\n",
        "            lines.append(\" \".join(current_line))\n",
        "            current_line = []\n",
        "            word_count = 0\n",
        "        elif word_count > 30:\n",
        "            lines.append(\" \".join(current_line))\n",
        "            current_line = []\n",
        "            word_count = 0\n",
        "\n",
        "    if current_line:\n",
        "        lines.append(\" \".join(current_line))\n",
        "\n",
        "    beautiful_text = \"\\n\".join(lines)\n",
        "    print(beautiful_text)\n"
      ],
      "metadata": {
        "id": "45WdtUZyB350"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(one_step_model, prompt):\n",
        "  states = None\n",
        "  next_char = tf.constant([prompt])\n",
        "  result = [next_char]\n",
        "\n",
        "  for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "\n",
        "  beautiful_print(result)"
      ],
      "metadata": {
        "id": "6zy4xcvf_-Gn"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2_load = MyModel(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "kacFKxVqKrsD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2_load.load_weights(\"/content/training_checkpoints/ckpt_20\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nlez4KLQK8Qj",
        "outputId": "bbaa515f-08b4-460f-e9b6-1759b5b15580"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b812e8f7310>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model2_load, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t43e6Fl3LLm1",
        "outputId": "c3e042f3-09cc-43aa-f40f-c2eb7108bd47"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0]], shape=(1, 1), dtype=int64)\n",
            "SparseTensor(indices=tf.Tensor([[0]], shape=(1, 1), dtype=int64), values=tf.Tensor([-inf], shape=(1,), dtype=float32), dense_shape=tf.Tensor([76], shape=(1,), dtype=int64))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(one_step_model, \"To be or not to be, that's the \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkAWlgvuLoA3",
        "outputId": "9bca2ee9-077b-4861-cf82-1c9346d4970c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be, that's the musto must be great: Who in prophric shall find him not: Be quite out to interprene her!\n",
            "O hating whether me to thy love? Good uncle, I must needs to London with thee whatsoe'er! Wert thou dread or for my part, Lord York, or no, lord abused shoot\n",
            "is inford! To lip his cave and master, where is great heasty To help, madness, I cannot sing in Argas' with close boys of trifles here.\n",
            "Wilt have of you being 'Tell me, fly, fly, from that very hour! I'll have my judge mine honesty: DON Are more fear me my sooner-bodies.\n",
            "The which you rush'd and to behold this nights themselves They may possession from the blood of his pohility. We will drink to me, fiend, my brother!\n",
            "Escans, shepherd, while it is twit 'ord of him. sword in her clamours how it was By his that they have power upon a rich Fellow, Comwell, charge thy imposition: we\n",
            "are breed True one on Brutus, let the infamy for thy pobe. Those that shall survice this That or our power were most needs one worst or gentlemen.\n",
            "or so: of your affec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the generated text, we'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "metadata": {
        "id": "Z0oc5s42gbZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We can also save and restore the generator model as follows\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "DDSXXyrEgqbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}